{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SMXp295AH40o",
        "outputId": "d9a478ec-5b29-44cf-d39f-42089c0c28c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_md"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmexXk5eICM5",
        "outputId": "981679ce-b63a-4b2a-c1a6-edecd019e647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-md==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.7.0/es_core_news_md-3.7.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-md==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocesar Documentos"
      ],
      "metadata": {
        "id": "jk7LPgnWEbSQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ySlGC8MeELfB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import json\n",
        "\n",
        "# Cargar el modelo de spaCy (usa el modelo adecuado para el idioma de los textos (español == es_core_news_md)\n",
        "nlp = spacy.load('en_core_web_md')  #                                            (inglés == en_core_web_md)\n",
        "\n",
        "# Configuración de stopwords y POS válidos\n",
        "valid_POS = {'VERB', 'NOUN', 'ADJ', 'PROPN'}\n",
        "stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "# Función de preprocesamiento\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesa un texto para prepararlo para técnicas como BoW o TF-IDF.\n",
        "    - Filtra stopwords\n",
        "    - Lematiza palabras\n",
        "    - Retiene solo palabras alfabéticas y ciertas POS\n",
        "\n",
        "    Args:\n",
        "        text (str): Texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        str: Texto procesado.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Procesar con spaCy (Tokenizar)\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # Filtrar y lematizar\n",
        "        lemmatized = [\n",
        "            token.lemma_.lower() for token in doc\n",
        "            if token.is_alpha and token.pos_ in valid_POS # Filtrar POS\n",
        "            and token.text.lower() not in stopwords # Elimina StopWords\n",
        "        ]\n",
        "        return \" \".join(lemmatized)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando texto: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Función para aplicar a todas las filas de un DataFrame\n",
        "def preprocess_dataframe(df, text_columns):\n",
        "    \"\"\"\n",
        "    Aplica el preprocesamiento a las columnas de texto seleccionadas en un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame con los datos.\n",
        "        text_columns (list): Lista de nombres de columnas a combinar y procesar.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame con una nueva columna 'processed_text'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Combinar columnas de texto en una sola\n",
        "        df['combined_text'] = df[text_columns].fillna('').apply(lambda row: ' '.join(row), axis=1)\n",
        "\n",
        "        # Aplicar el preprocesamiento\n",
        "        df['processed_text'] = df['combined_text'].apply(preprocess_text)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando DataFrame: {e}\")\n",
        "        return df\n",
        "\n",
        "def save_processed_text_to_file(df, column_name, file_name):\n",
        "    \"\"\"\n",
        "    Guarda los textos preprocesados de un DataFrame en un archivo .txt.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame que contiene la columna con textos preprocesados.\n",
        "        column_name (str): Nombre de la columna que contiene los textos preprocesados.\n",
        "        file_name (str): Nombre del archivo donde guardar los textos.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Abrir el archivo en modo escritura\n",
        "        with open(file_name, 'w', encoding='utf-8') as f:\n",
        "            # Escribir cada texto en una línea separada\n",
        "            for text in df[column_name]:\n",
        "                f.write(text + '\\n')\n",
        "        print(f\"Textos guardados en el archivo: {file_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar en archivo: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Cargar y Preprocesar todos los datos del dataset (Cambiar rutas y archivos para los datos)"
      ],
      "metadata": {
        "id": "2IiKFRTmR8Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta al archivo en tu Google Drive\n",
        "file_path = '/content/drive/MyDrive/MÁSTER/NLP/arxiv_papers.csv'\n",
        "\n",
        "# Cargar el dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Ver las primeras filas del dataset\n",
        "print(\"Primeras filas del dataset:\")\n",
        "print(df.head())\n",
        "text_columns = ['title', 'abstract']\n",
        "\n",
        "# Aplicar preprocesamiento a estas columnas\n",
        "preprocessed_df = preprocess_dataframe(df, text_columns=text_columns)\n",
        "save_processed_text_to_file(preprocessed_df, column_name='processed_text', file_name='processed_texts.txt')\n",
        "\n",
        "# Mostrar las primeras filas después del preprocesamiento\n",
        "print(\"Primeras filas después del preprocesamiento:\")\n",
        "print(preprocessed_df[['title', 'processed_text']])"
      ],
      "metadata": {
        "id": "eQhhhOF-VU_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7a6951-2599-44ee-bb2b-a38eaf246f1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Primeras filas del dataset:\n",
            "                                               title  \\\n",
            "0    MetaFormer is Actually What You Need for Vision   \n",
            "1     Turbo Autoencoder with a Trainable Interleaver   \n",
            "2  Ab-initio calculation of point defect equilibr...   \n",
            "3  Divergent electrostriction at ferroelectric ph...   \n",
            "4  ProxyFL: Decentralized Federated Learning thro...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  Transformers have shown great potential in com...   \n",
            "1  A critical aspect of reliable communication in...   \n",
            "2  Point defects are responsible for a wide range...   \n",
            "3  We investigate the electrostrictive response a...   \n",
            "4  Institutions in highly regulated domains such ...   \n",
            "\n",
            "                   published  \\\n",
            "0  2021-11-22T18:52:03+00:00   \n",
            "1  2021-11-22T18:37:03+00:00   \n",
            "2  2021-11-22T17:11:17+00:00   \n",
            "3  2021-11-22T17:00:32+00:00   \n",
            "4  2021-11-22T16:47:39+00:00   \n",
            "\n",
            "                                             authors  \\\n",
            "0  ['Weihao Yu', 'Mi Luo', 'Pan Zhou', 'Chenyang ...   \n",
            "1  ['Karl Chahine', 'Yihan Jiang', 'Pooja Nuti', ...   \n",
            "2  ['Mubashir Mansoor', 'Mehya Mansoor', 'Maryam ...   \n",
            "3  ['Daniel S. P. Tanner', 'Pierre-Eymeric Janoli...   \n",
            "4  ['Shivam Kalra', 'Junfeng Wen', 'Jesse C. Cres...   \n",
            "\n",
            "                                 url  \n",
            "0  http://arxiv.org/abs/2111.11418v1  \n",
            "1  http://arxiv.org/abs/2111.11410v1  \n",
            "2  http://arxiv.org/abs/2111.11359v1  \n",
            "3  http://arxiv.org/abs/2111.11352v1  \n",
            "4  http://arxiv.org/abs/2111.11343v1  \n",
            "Textos guardados en el archivo: processed_texts.txt\n",
            "Primeras filas después del preprocesamiento:\n",
            "                                                  title  \\\n",
            "0       MetaFormer is Actually What You Need for Vision   \n",
            "1        Turbo Autoencoder with a Trainable Interleaver   \n",
            "2     Ab-initio calculation of point defect equilibr...   \n",
            "3     Divergent electrostriction at ferroelectric ph...   \n",
            "4     ProxyFL: Decentralized Federated Learning thro...   \n",
            "...                                                 ...   \n",
            "7995  Differentiable Random Access Memory using Latt...   \n",
            "7996  An open-endcap blade trap for radial-2D ion cr...   \n",
            "7997  An Empirical Analysis of VM Startup Times in P...   \n",
            "7998  A repeated-measures study on emotional respons...   \n",
            "7999  Anticipating Safety Issues in E2E Conversation...   \n",
            "\n",
            "                                         processed_text  \n",
            "0     metaformer need vision transformers show great...  \n",
            "1     turbo autoencoder trainable interleaver critic...  \n",
            "2     ab initio calculation point defect equilibrium...  \n",
            "3     divergent electrostriction ferroelectric phase...  \n",
            "4     proxyfl decentralized federated learning proxy...  \n",
            "...                                                 ...  \n",
            "7995  differentiable random access memory lattice in...  \n",
            "7996  open endcap blade trap ion crystal present des...  \n",
            "7997  empirical analysis vm startup times public iaa...  \n",
            "7998  repeat measure study emotional response year p...  \n",
            "7999  anticipating safety issues conversational ai f...  \n",
            "\n",
            "[8000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Cargando Nuestros Datos"
      ],
      "metadata": {
        "id": "AxMZqfxHR-5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta al archivo JSON en tu Drive\n",
        "file_path = '/content/drive/MyDrive/MÁSTER/NLP/arxiv-metadata-oai-snapshot.json'\n",
        "\n",
        "# Cargar datos desde el archivo JSON\n",
        "data = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Crear un DataFrame con las columnas relevantes\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Seleccionar las columnas que nos interesan para análisis\n",
        "# En este caso, usaremos 'authors', 'title', y 'abstract'\n",
        "df = df[['authors', 'title', 'abstract']]\n",
        "\n",
        "# Renombrar las columnas para consistencia con el preprocesamiento\n",
        "df.rename(columns={'authors': 'author'}, inplace=True)\n",
        "\n",
        "# Verificar el DataFrame cargado\n",
        "print(df.head())\n",
        "\n",
        "# Aplicar la función de preprocesamiento\n",
        "preprocessed_df = preprocess_dataframe(df, text_columns=['title', 'abstract'])\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(preprocessed_df[['author', 'processed_text']])\n",
        "save_processed_text_to_file(preprocessed_df, column_name='processed_text', file_name='processed_texts.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "yyOlimRPSFcO",
        "outputId": "1a4a8207-e40a-4699-825f-9947da561f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Extra data: line 2 column 1 (char 1689)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b46e700c39e6>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#for line in file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 1689)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Crear un Corpus"
      ],
      "metadata": {
        "id": "m1SQoVyDL1Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mycorpus = open('processed_texts.txt').readlines()\n",
        "mycorpus = [el.strip().split() for el in mycorpus]\n",
        "\n",
        "print(('Number of documents in corpus: '+str(len(mycorpus))))\n",
        "print(('============= First document in corpus ============='))\n",
        "print(mycorpus[0])\n",
        "print(('============= Corresponding Python string ============='))\n",
        "print(' '.join(mycorpus[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XxMqtgtL2w1",
        "outputId": "70dc90b2-2ba5-4ef8-ce33-9f41f2eb8581"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents in corpus: 8000\n",
            "============= First document in corpus =============\n",
            "['metaformer', 'need', 'vision', 'transformers', 'show', 'great', 'potential', 'computer', 'vision', 'task', 'common', 'belief', 'attention', 'base', 'token', 'mixer', 'module', 'contribute', 'competence', 'recent', 'work', 'attention', 'base', 'module', 'transformer', 'replace', 'spatial', 'mlp', 'resulted', 'model', 'perform', 'base', 'observation', 'hypothesize', 'general', 'architecture', 'transformer', 'specific', 'token', 'mixer', 'module', 'essential', 'model', 'performance', 'verify', 'replace', 'attention', 'module', 'transformer', 'simple', 'spatial', 'pooling', 'operator', 'conduct', 'basic', 'token', 'mixing', 'observe', 'derive', 'model', 'term', 'poolformer', 'achieve', 'competitive', 'performance', 'multiple', 'computer', 'vision', 'task', 'example', 'k', 'poolformer', 'achieve', 'accuracy', 'surpass', 'tune', 'vision', 'transformer', 'mlp', 'like', 'baseline', 'deit', 'b', 'resmlp', 'accuracy', 'few', 'parameter', 'few', 'mac', 'effectiveness', 'poolformer', 'verify', 'hypothesis', 'urge', 'initiate', 'concept', 'metaformer', 'general', 'architecture', 'abstract', 'transformer', 'specify', 'token', 'mixer', 'base', 'extensive', 'experiment', 'argue', 'metaformer', 'key', 'player', 'achieve', 'superior', 'result', 'recent', 'transformer', 'mlp', 'like', 'model', 'vision', 'task', 'work', 'call', 'future', 'research', 'dedicate', 'improve', 'metaformer', 'focus', 'token', 'mixer', 'module', 'propose', 'poolformer', 'serve', 'starting', 'baseline', 'future', 'metaformer', 'architecture', 'design', 'code', 'available']\n",
            "============= Corresponding Python string =============\n",
            "metaformer need vision transformers show great potential computer vision task common belief attention base token mixer module contribute competence recent work attention base module transformer replace spatial mlp resulted model perform base observation hypothesize general architecture transformer specific token mixer module essential model performance verify replace attention module transformer simple spatial pooling operator conduct basic token mixing observe derive model term poolformer achieve competitive performance multiple computer vision task example k poolformer achieve accuracy surpass tune vision transformer mlp like baseline deit b resmlp accuracy few parameter few mac effectiveness poolformer verify hypothesis urge initiate concept metaformer general architecture abstract transformer specify token mixer base extensive experiment argue metaformer key player achieve superior result recent transformer mlp like model vision task work call future research dedicate improve metaformer focus token mixer module propose poolformer serve starting baseline future metaformer architecture design code available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Detectar N-Grams (Opcional, si queremos hacerlo debemos ajustar los thresholds para detectar buenos N-Grams)"
      ],
      "metadata": {
        "id": "NDQQEA7cMpa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases\n",
        "\n",
        "phrase_model = Phrases(mycorpus, min_count=2, threshold=20)\n",
        "mycorpus = [el for el in phrase_model[mycorpus]] #We populate mycorpus again\n",
        "print(('============= First document after N-gram replacement ============='))\n",
        "print(mycorpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LSMJQ_dMr8r",
        "outputId": "d8404c72-0e28-453b-9890-c361406bcf92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= First document after N-gram replacement =============\n",
            "['metaformer', 'need', 'vision_transformers', 'show_great', 'potential', 'computer_vision', 'task', 'common', 'belief', 'attention', 'base', 'token_mixer', 'module', 'contribute', 'competence', 'recent', 'work', 'attention', 'base', 'module', 'transformer', 'replace', 'spatial', 'mlp', 'resulted', 'model', 'perform', 'base', 'observation', 'hypothesize', 'general', 'architecture', 'transformer', 'specific', 'token_mixer', 'module', 'essential', 'model', 'performance', 'verify', 'replace', 'attention_module', 'transformer', 'simple', 'spatial', 'pooling', 'operator', 'conduct', 'basic', 'token_mixing', 'observe', 'derive', 'model', 'term', 'poolformer', 'achieve_competitive', 'performance', 'multiple', 'computer_vision', 'task', 'example', 'k', 'poolformer', 'achieve', 'accuracy', 'surpass', 'tune', 'vision_transformer', 'mlp', 'like', 'baseline', 'deit', 'b', 'resmlp', 'accuracy', 'few_parameter', 'few', 'mac', 'effectiveness', 'poolformer', 'verify', 'hypothesis', 'urge', 'initiate', 'concept', 'metaformer', 'general', 'architecture', 'abstract', 'transformer', 'specify', 'token_mixer', 'base', 'extensive_experiment', 'argue', 'metaformer', 'key', 'player', 'achieve_superior', 'result', 'recent', 'transformer_mlp', 'like', 'model', 'vision_task', 'work', 'call', 'future_research', 'dedicate', 'improve', 'metaformer', 'focus', 'token_mixer', 'module', 'propose', 'poolformer', 'serve', 'starting', 'baseline', 'future', 'metaformer', 'architecture', 'design', 'code_available']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Creamos un Diccionario"
      ],
      "metadata": {
        "id": "Si2olaYAN6Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "no_below = 4 #Minimum number of documents to keep a term in the dictionary\n",
        "no_above = .80 #Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
        "\n",
        "# Create dictionary of tokens\n",
        "D = Dictionary(mycorpus)\n",
        "D.filter_extremes(no_below=no_below,no_above=no_above)\n",
        "\n",
        "n_tokens = len(D)\n",
        "\n",
        "print('The dictionary contains', n_tokens, 'terms')\n",
        "print('First terms in the dictionary:')\n",
        "for n in range(10):\n",
        "    print(str(n), ':', D[n])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6Q-oPGBNzpG",
        "outputId": "1d42e69f-3b40-4fc5-8e0f-cf532f73b842"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dictionary contains 11654 terms\n",
            "First terms in the dictionary:\n",
            "0 : abstract\n",
            "1 : accuracy\n",
            "2 : achieve\n",
            "3 : achieve_competitive\n",
            "4 : achieve_superior\n",
            "5 : architecture\n",
            "6 : argue\n",
            "7 : attention\n",
            "8 : attention_module\n",
            "9 : b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Hacemos BoW"
      ],
      "metadata": {
        "id": "VS6k6BkcOOXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mycorpus_bow = [D.doc2bow(doc) for doc in mycorpus]\n",
        "\n",
        "n_project = 1000\n",
        "print(('============= Project abstract (lemmas) ============='))\n",
        "print(' '.join(mycorpus[n_project]))\n",
        "\n",
        "print(('============= Sparse vector representation ============='))\n",
        "print(mycorpus_bow[n_project])\n",
        "\n",
        "print(('============= Word counts for the project ============='))\n",
        "print(list(map(lambda x: (D[x[0]], x[1]), mycorpus_bow[n_project])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J32GyWwAOQVa",
        "outputId": "d24a0d26-5d5f-43e1-f4b0-92f15458b5cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Project abstract (lemmas) =============\n",
            "real_time simulation level level level electric_vehicle charging systems charge system require convert ac electricity grid dc electricity charge electric_vehicle ev_battery accord society automatic engineers sae standard ev charger divide level base power rating level level level paper investigate circuit topology control principle ev_charge system level high_fidelity testbed ev_charge system kwh battery design implement real_time digital simulator rt lab testbed include model detail switch semiconductor minute real_time simulation conduct testbed detailed dynamic performance circuit control stage present demonstrate charge process level ev_charge system employ high frequency transformer embed dual active bridge dab dc_dc converter regulate battery dc_voltage current average model base linear system analysis give configure parameter phase_shift control adopt dab dc_dc converter addition power factor control pfc employ level level single phase ac charge system phase voltage source converter control employ level phase ac charge system analyze testbed detailed circuit parameter control parameter present reference testbed ev grid integration research\n",
            "============= Sparse vector representation =============\n",
            "[(10, 2), (19, 1), (42, 2), (50, 1), (68, 1), (80, 1), (94, 1), (113, 1), (163, 1), (175, 3), (185, 1), (186, 1), (194, 7), (224, 3), (283, 1), (305, 1), (308, 1), (315, 2), (330, 1), (333, 6), (364, 3), (365, 1), (447, 1), (454, 1), (472, 1), (540, 1), (555, 1), (556, 3), (562, 1), (565, 1), (582, 2), (612, 1), (622, 1), (661, 1), (677, 12), (683, 2), (718, 1), (749, 1), (794, 1), (817, 1), (822, 1), (830, 1), (994, 1), (998, 5), (1013, 2), (1015, 1), (1148, 1), (1296, 1), (1392, 1), (1579, 1), (1683, 2), (1695, 1), (1745, 1), (1968, 1), (2123, 5), (2178, 1), (2185, 1), (2190, 1), (2191, 1), (2236, 1), (2681, 3), (2834, 1), (2868, 2), (3185, 1), (3321, 1), (3509, 1), (3532, 1), (4192, 1), (4242, 1), (4300, 1), (4807, 1), (4976, 1), (5021, 2), (5141, 1), (5527, 1), (5643, 2), (6634, 3), (7040, 3), (7060, 1), (7446, 2), (8340, 2), (8341, 1), (8342, 1), (8343, 1), (8344, 1), (8345, 1)]\n",
            "============= Word counts for the project =============\n",
            "[('base', 2), ('conduct', 1), ('model', 2), ('performance', 1), ('transformer', 1), ('analysis', 1), ('demonstrate', 1), ('paper', 1), ('investigate', 1), ('parameter', 3), ('principle', 1), ('process', 1), ('system', 7), ('phase', 3), ('regulate', 1), ('high', 1), ('linear', 1), ('power', 2), ('automatic', 1), ('control', 6), ('real_time', 3), ('research', 1), ('single', 1), ('addition', 1), ('include', 1), ('stage', 1), ('current', 1), ('employ', 3), ('give', 1), ('integration', 1), ('simulation', 2), ('analyze', 1), ('frequency', 1), ('detail', 1), ('level', 12), ('present', 2), ('implement', 1), ('embed', 1), ('dynamic', 1), ('require', 1), ('standard', 1), ('accord', 1), ('active', 1), ('charge', 5), ('ev', 2), ('factor', 1), ('source', 1), ('adopt', 1), ('systems', 1), ('switch', 1), ('grid', 2), ('reference', 1), ('dual', 1), ('convert', 1), ('testbed', 5), ('voltage', 1), ('bridge', 1), ('digital', 1), ('divide', 1), ('lab', 1), ('circuit', 3), ('topology', 1), ('detailed', 2), ('semiconductor', 1), ('rating', 1), ('average', 1), ('dc', 1), ('minute', 1), ('configure', 1), ('simulator', 1), ('society', 1), ('rt', 1), ('electric_vehicle', 2), ('high_fidelity', 1), ('phase_shift', 1), ('battery', 2), ('ac', 3), ('converter', 3), ('charging', 1), ('electricity', 2), ('dc_dc', 2), ('dc_voltage', 1), ('engineers', 1), ('kwh', 1), ('pfc', 1), ('sae', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Crear LDA"
      ],
      "metadata": {
        "id": "ICnaUwvYOp0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "num_topics = 20\n",
        "\n",
        "ldag = LdaModel(corpus=mycorpus_bow, id2word=D, num_topics=num_topics)"
      ],
      "metadata": {
        "id": "1Om0L-_OOsR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a287d8-72d5-4d58-dd96-d9a510b95c9e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Encontrar los documentos más relevantes de un tópic"
      ],
      "metadata": {
        "id": "daqzSguyQ-ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_relevant_documents(ldag, topicid, corpus_bow, ndocs=10):\n",
        "    \"\"\"This function returns the most relevant documents in corpus_bow\n",
        "\n",
        "    : ldag: The trained topic model object provided by gensim\n",
        "    : topicid: The topic for which we want to find the most relevant documents\n",
        "    : corpus_bow: The BoW representation of documents in Gensim format\n",
        "    : ndocs: Number of most relevant documents to return\n",
        "\n",
        "    : Returns: A list with the identifiers of the most relevant documents\n",
        "    \"\"\"\n",
        "    print('Computing most relevant documents for Topic', topicid)\n",
        "    print('Topic composition is:')\n",
        "    print(ldag.show_topic(topicid))\n",
        "\n",
        "    # Compute relevance of each document for the given topic\n",
        "    doc_topic_probs = [\n",
        "        (doc_id, ldag.get_document_topics(bow, minimum_probability=0)[topicid][1])\n",
        "        for doc_id, bow in enumerate(corpus_bow)\n",
        "    ]\n",
        "\n",
        "    # Sort documents by their probability for the topic in descending order\n",
        "    sorted_docs = sorted(doc_topic_probs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract the identifiers of the most relevant documents\n",
        "    most_relevant_doc_ids = [doc_id for doc_id, _ in sorted_docs[:ndocs]]\n",
        "\n",
        "    return most_relevant_doc_ids\n",
        "\n",
        "# To test the function we will find the most relevant projects for one of the topics\n",
        "project_id = most_relevant_documents(ldag, 8, mycorpus_bow, ndocs=3)\n",
        "\n",
        "# Print titles of selected projects\n",
        "for idproject in project_id:\n",
        "    print('\\n', ' '.join(mycorpus[idproject]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a95aqGY4RFr0",
        "outputId": "af73bc23-9850-468b-d47e-cc03799c3981"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing most relevant documents for Topic 8\n",
            "Topic composition is:\n",
            "[('algorithm', 0.031518213), ('problem', 0.0163214), ('propose', 0.014928381), ('base', 0.0105038015), ('method', 0.01007682), ('learning', 0.008250329), ('network', 0.007867106), ('result', 0.0076906467), ('approach', 0.0063222665), ('paper', 0.0062838783)]\n",
            "\n",
            " distribute plug n play algorithm multi robot application priori non computable objective_function paper_present distribute algorithm applicable_wide range practical multi robot application multi robot application user define objective mission cast general optimization_problem explicit guideline subtask different robot owe unknown environment unknown robot dynamic sensor nonlinearitie analytic form optimization cost function available standard gradient_descent like algorithm applicable problem tackle introduce new algorithm design robot subcost function optimization accomplish overall team objective transformation propose distribute methodology base base adaptive optimization cao algorithm able approximate evolution robot cost function optimize decision variable robot action achieve online learn problem specific characteristic affect accomplishment mission objective overall low_complexity algorithm incorporate kind operational_constraint fault_tolerant tackle time_vary cost function cornerstone approach share convergence characteristic block_coordinate descent algorithm propose algorithm evaluate heterogeneous simulation set_up multiple scenario general_purpose problem specific algorithm source_code available athakapo distribute plug n play algorithm multi robot application\n",
            "\n",
            " online facility_location linear delay study problem online facility_location delay problem sequence client appear metric space need connect open facility client connect choice come penalty client incur waiting cost difference arrival connection time point time algorithm decide open facility connect subset_client study problem general class network design problem delay main focus new variant problem client connect open facility action incur extra cost algorithm pay wait facility cost_incur late connection reminiscent online matching delay side connection incur waiting cost variant sided_delay differentiate study sided_delay present deterministic algorithm sided_delay variant technical study greedy strategy grow budget increase waiting delay open facility subset_client sum budget reach certain_threshold technique substantial extension approach jain mahdian saberi stoc analyze performance offline algorithm facility_location transform algorithm sided_delay variant n deterministic algorithm sided_delay note previous online algorithm problem delay general metric logarithmic ratio\n",
            "\n",
            " hardness analysis thompson_sampling combinatorial semi bandit greedy_oracle thompson_sampling ts attract_lot interest bandit area introduce prove recent_year analysis combinatorial multi_armed bandit cmab setting require exact oracle provide optimal_solution input oracle feasible combinatorial_optimization problem_np hard approximation_oracle available example wang chen show failure ts learn approximation_oracle oracle uncommon design specific problem instance open_question convergence analysis ts extend exact oracle cmab paper study question greedy_oracle common approximation_oracle theoretical_guarantee solve offline combinatorial_optimization problem provide problem dependent regret_bind order quantify hardness ts solve cmab problem greedy_oracle time_horizon reward gap provide matching regret_bind theoretical result ts solve cmab common approximation_oracle break misconception ts work approximation_oracle\n"
          ]
        }
      ]
    }
  ]
}